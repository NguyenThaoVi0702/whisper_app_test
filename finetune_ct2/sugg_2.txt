
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | ==========
lora-finetuning-job-s3  | == CUDA ==
lora-finetuning-job-s3  | ==========
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | CUDA Version 12.0.1
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | This container image and its contents are governed by the NVIDIA Deep Learning Container License.
lora-finetuning-job-s3  | By pulling and using the container, you accept the terms and conditions of this license:
lora-finetuning-job-s3  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | =========================================
lora-finetuning-job-s3  |       STARTING FINE-TUNING PIPELINE
lora-finetuning-job-s3  | =========================================
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | --- Step 1: Preparing Data (Downloading, Augmenting, Caching) ---
lora-finetuning-job-s3  | 2025-10-31 10:01:58,528 - INFO - Pre-processed data already found on disk. Skipping data preparation.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | --- Step 2: Training Model (Loading Cached Data & Fine-Tuning) ---
lora-finetuning-job-s3  | 2025-10-31 10:02:04,071 - INFO - Loading pre-processed training data from /app/processed_data/train...
lora-finetuning-job-s3  | 2025-10-31 10:02:04,078 - INFO - Loading pre-processed validation data from /app/processed_data/validation...
lora-finetuning-job-s3  | 2025-10-31 10:02:04,080 - INFO - Datasets loaded successfully.
lora-finetuning-job-s3  | 2025-10-31 10:02:04,080 - INFO - Loading base model and applying LoRA adapter...
lora-finetuning-job-s3  | 2025-10-31 10:02:04,400 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
lora-finetuning-job-s3  | 2025-10-31 10:02:05,462 - INFO - Unfreezing convolutional layers for hybrid fine-tuning...
lora-finetuning-job-s3  | Traceback (most recent call last):
lora-finetuning-job-s3  |   File "/app/train_model.py", line 128, in <module>
lora-finetuning-job-s3  | trainable params: 11,962,880 || all params: 815,431,680 || trainable%: 1.4671
lora-finetuning-job-s3  |     main()
lora-finetuning-job-s3  |   File "/app/train_model.py", line 84, in main
lora-finetuning-job-s3  |     training_args = Seq2SeqTrainingArguments(
lora-finetuning-job-s3  | TypeError: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
lora-finetuning-job-s3 exited with code 1
