Of course. This is the perfect next step for debugging. By comparing the outputs of the base model, your prepared labels, and your original labels, you will see exactly where the discrepancies are.

Here is the code for the data checking script and a detailed guide on how to run it as a one-off command with Docker Compose.

1. The Data Checking Script

Save the following code into a new file named check_data.py in the same directory as your docker-compose.yml file.

code
Python
download
content_copy
expand_less
# check_data.py

import os
import json
import io
import logging
import re
import random

import boto3
import torch
import librosa
from datasets import load_from_disk, Dataset
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# --- Basic Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Suppress noisy warnings from other libraries
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("boto3").setLevel(logging.ERROR)
logging.getLogger("botocore").setLevel(logging.ERROR)

# --- Configuration Paths (Must match your other scripts) ---
ANNOTATIONS_FILE_IN_CONTAINER = "/app/annotations.json"
BASE_MODEL_PATH = "./model"
PROCESSED_TRAIN_PATH = "/app/processed_data/train"

# --- S3 Client Function (Copied from prepare_data.py) ---
def get_s3_client():
    """Initializes and returns a boto3 S3 client."""
    try:
        return boto3.client(
            "s3",
            endpoint_url=os.environ.get("S3_ENDPOINT_URL"),
            aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY"),
            region_name=os.environ.get("AWS_REGION"),
        )
    except Exception as e:
        raise ConnectionError(f"Failed to create S3 client: {e}")

# --- Data Loading Function (Copied from prepare_data.py) ---
def load_and_parse_annotations(annotations_path):
    with open(annotations_path, 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    data = {"audio_path": [], "transcription": []}
    for task in annotations:
        s3_uri = task.get("data", {}).get("audio")
        if not s3_uri: continue
        segments = []
        if task.get("annotations"):
            for result in task["annotations"][0].get("result", []):
                if result.get("from_name") == "transcription":
                    start_time = result.get("value", {}).get("start", 0)
                    text = " ".join(result.get("value", {}).get("text", [])).strip()
                    label = "unknown"
                    for label_result in task["annotations"][0].get("result", []):
                        if label_result.get("id") == result.get("id") and label_result.get("from_name") == "label":
                           label = " ".join(label_result.get("value",{}).get("labels",[]))
                           break
                    if "noise" not in label.lower() and text:
                        segments.append((start_time, text))
        if not segments: continue
        segments.sort(key=lambda x: x[0])
        full_transcription = " ".join([text for _, text in segments])
        data["audio_path"].append(s3_uri)
        data["transcription"].append(full_transcription)
    return Dataset.from_dict(data)

def main():
    """
    Loads 5 random samples, compares the original label, the processed label,
    and a fresh transcription from the base model.
    """
    try:
        s3_client = get_s3_client()
        
        logging.info("Loading base model and processor...")
        processor = WhisperProcessor.from_pretrained(BASE_MODEL_PATH, language="vi", task="transcribe")
        model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_PATH, torch_dtype=torch.bfloat16)
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        logging.info(f"Model loaded onto device: {device}")

        logging.info("Loading processed and raw datasets...")
        # Load the dataset that the model was actually trained on
        train_dataset_processed = load_from_disk(PROCESSED_TRAIN_PATH)

        # Load the raw dataset and perform the same split to align the samples
        raw_dataset = load_and_parse_annotations(ANNOTATIONS_FILE_IN_CONTAINER)
        dataset_dict = raw_dataset.train_test_split(test_size=0.1, seed=42)
        train_dataset_raw = dataset_dict["train"]
        
        logging.info(f"Checking {min(5, len(train_dataset_processed))} random samples...")
        
        # Get 5 unique random indices
        random_indices = random.sample(range(len(train_dataset_processed)), k=min(5, len(train_dataset_processed)))

        for i, idx in enumerate(random_indices):
            print("\n" + "="*80)
            print(f"SAMPLE {i+1}/{len(random_indices)} (Dataset Index: {idx})")
            print("="*80)

            # --- 1. Get data from your processed and raw datasets ---
            processed_sample = train_dataset_processed[idx]
            raw_sample = train_dataset_raw[idx]
            
            s3_uri = raw_sample["audio_path"]
            
            # Decode the label that your model was trained on
            normalized_label = processor.tokenizer.decode(processed_sample['labels'], skip_special_tokens=True)
            
            # Get the original, un-normalized label
            original_label = raw_sample['transcription']
            
            # --- 2. Download audio and run inference with the base model ---
            logging.info(f"Downloading and transcribing audio from {s3_uri}...")
            bucket = s3_uri.split('/')[2]
            key = '/'.join(s3_uri.split('/')[3:])
            
            response = s3_client.get_object(Bucket=bucket, Key=key)
            audio_bytes = response['Body'].read()
            audio_array, _ = librosa.load(io.BytesIO(audio_bytes), sr=16000, mono=True)

            input_features = processor(audio_array, sampling_rate=16000, return_tensors="pt").input_features
            predicted_ids = model.generate(input_features.to(device))
            base_model_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

            # --- 3. Print the comparison ---
            print(f"\n[A] BASE MODEL'S GUESS (Inference):")
            print(f"    '{base_model_transcription}'")
            
            print(f"\n[B] YOUR PROCESSED LABEL (What your model was trained on):")
            print(f"    '{normalized_label}'")

            print(f"\n[C] YOUR ORIGINAL LABEL (From annotations.json):")
            print(f"    '{original_label}'")
            
    except Exception as e:
        logging.error(f"An error occurred: {e}", exc_info=True)

if __name__ == "__main__":
    main()```

### 2. How to Run the Script

You can run this new script inside a new container using your existing `finetuner` service definition without modifying `docker-compose.yml` at all. The command for this is `docker compose run`.

**Step 1: Save the File**
Make sure the code above is saved as `check_data.py` in your project's root directory.

**Step 2: Run the Command**
Open your terminal in the same directory and execute the following command:

```bash
docker compose run --rm finetuner python check_data.py

How this command works:

docker compose run: Starts a new, one-off container for a service.

--rm: Automatically removes the container after the script finishes, keeping your system clean.

finetuner: The name of the service from your docker-compose.yml. This tells Docker to use the same image, volumes, environment variables, and GPU settings defined for that service.

python check_data.py: This is the command that will be executed inside the new container, overriding the default command: /bin/bash run_pipeline.sh.

3. How to Interpret the Output

You will see output for 5 random samples, each formatted like this:

code
Code
download
content_copy
expand_less
================================================================================
SAMPLE 1/5 (Dataset Index: 1234)
================================================================================

[A] BASE MODEL'S GUESS (Inference):
    'xin chào thế giới'

[B] YOUR PROCESSED LABEL (What your model was trained on):
    'xin chào thế giới'

[C] YOUR ORIGINAL LABEL (From annotations.json):
    'Xin chào, Thế Giới.'

Analyze the differences:

Compare (A) and (B):

If they are very different: It could mean the base audio quality is poor or the domain is very different from Whisper's pre-training data.

If they are similar: This is a good sign! It means the base model can already understand your audio well.

Compare (B) and (C):

This is the most important comparison. It shows the effect of your normalization. You should see [C] having punctuation and capitalization, while [B] is clean, lowercase, and free of symbols.

If [B] and [C] are identical (e.g., both have punctuation), your normalization fix was not applied correctly.

Overall Picture: Your goal is to make [B] (your processed label) a clean, accurate, and normalized version of the spoken audio. The closer [B] is to a perfect transcription, the better your fine-tuned model will be. This script gives you the exact insight you need to verify that critical step.
