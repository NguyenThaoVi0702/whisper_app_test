# train_model.py

# ... (imports and global processor are the same) ...

# --- Data Collator and Metrics (unchanged) ---
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    # ... (no changes here)
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

def compute_metrics(pred):
    # ... (no changes here)
    pred_ids = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    wer = jiwer.wer(label_str, pred_str)
    return {"wer": wer}


def main():
    # ... (Loading data is the same) ...
    logging.info(f"Loading pre-processed training data from {PROCESSED_TRAIN_PATH}...")
    train_dataset = load_from_disk(PROCESSED_TRAIN_PATH)
    logging.info(f"Loading pre-processed validation data from {PROCESSED_VAL_PATH}...")
    val_dataset = load_from_disk(PROCESSED_VAL_PATH)
    logging.info("Datasets loaded successfully.")

    # 3. Model Loading and Hybrid Fine-Tuning Setup
    logging.info("Loading base model and applying LoRA adapter...")
    model = WhisperForConditionalGeneration.from_pretrained(
        BASE_MODEL_PATH, device_map="auto", use_cache=False, torch_dtype=torch.bfloat16
    )
    
    ### MODIFIED SECTION ###
    # The order of operations is critical for avoiding CUDA memory errors
    # with some library versions.
    
    # Step 3a: Prepare the model for k-bit training *before* applying the adapter.
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True, gradient_checkpointing_kwargs={"use_reentrant": False})
    
    # Step 3b: Now, load the LoRA adapter onto the prepared model.
    model = PeftModel.from_pretrained(model, ADAPTER_TO_CONTINUE_FROM, is_trainable=True)
    
    # Step 3c: Finally, unfreeze the conv layers for our hybrid strategy.
    logging.info("Unfreezing convolutional layers for hybrid fine-tuning...")
    for name, param in model.model.model.encoder.named_parameters():
        if "conv" in name:
            param.requires_grad = True
            
    model.print_trainable_parameters()
    ### END MODIFIED SECTION ###


    # 4. Training Arguments
    # ... (This section is unchanged)
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    has_bf16 = torch.cuda.is_bf16_supported()
    training_args = Seq2SeqTrainingArguments(
        output_dir=NEW_ADAPTER_SAVE_PATH,
        per_device_train_batch_size=16,
        gradient_accumulation_steps=4,
        learning_rate=5e-6,
        warmup_steps=50,
        num_train_epochs=5,
        bf16=has_bf16,
        fp16=not has_bf16,
        optim="adamw_torch",
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        logging_steps=10,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        dataloader_num_workers=4,
        generation_max_length=448,
    )

    # 5. Initialize and Run Trainer
    # ... (This section is unchanged)
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        processing_class=processor,
    )
    logging.info("Starting model training...")
    trainer.train()

    logging.info("Final evaluation on the best model:")
    final_metrics = trainer.evaluate()
    logging.info(f"Final WER: {final_metrics['eval_wer']:.4f}")

    logging.info(f"Training complete. Best adapter saved to {NEW_ADAPTER_SAVE_PATH}")
    trainer.save_model()

if __name__ == "__main__":
    main()
