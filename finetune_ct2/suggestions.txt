Of course. This is an excellent and professional way to structure a machine learning pipeline. The slow data preparation is a one-time cost, and you should absolutely cache that work so you can experiment with the model training part quickly and repeatedly without penalty.

We will create a robust, two-stage pipeline.

prepare_data.py: This script will do the slow work. It will connect to S3, download all audio, apply augmentations, process the text, and save the final, model-ready dataset to disk in a highly efficient format.

train_model.py: This script will be fast to start. It will assume the data is already prepared. It will load the pre-processed dataset from disk, load the model, and focus exclusively on the training loop.

run_pipeline.sh: A new, simple script that runs these two Python scripts in sequence.

This structure means if train_model.py fails for any reason (e.g., you want to try a different learning rate), you can tweak it and re-run the pipeline. The script will fly through the first step because the data is already saved, and you'll be back to training in seconds.

Step 1: Create the New Directories and Files

First, let's set up the directory structure. In your project folder, create a new directory for the cached data:

code
Bash
download
content_copy
expand_less
mkdir processed_data

Now, create the three new files: prepare_data.py, train_model.py, and run_pipeline.sh.

Step 2: The prepare_data.py Script

This script's only job is to prepare and save the data. It contains all the S3, audio, and augmentation logic.

code
Python
download
content_copy
expand_less
# prepare_data.py

import os
import json
import io
import logging
import threading

import boto3
import librosa
from datasets import Dataset
from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Gain
from transformers import WhisperProcessor

# --- Basic Setup ---
os.environ["TOKENIZERS_PARALLELISM"] = "false"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration Paths ---
ANNOTATIONS_FILE_IN_CONTAINER = "/app/annotations.json"
BASE_MODEL_PATH = "./model"
# The output directories for our cached, processed data
PROCESSED_TRAIN_PATH = "/app/processed_data/train"
PROCESSED_VAL_PATH = "/app/processed_data/validation"

# --- S3 Client Setup ---
def get_s3_client():
    """Initializes and returns a boto3 S3 client."""
    try:
        s3_client = boto3.client(
            "s3",
            endpoint_url=os.environ.get("S3_ENDPOINT_URL"),
            aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY"),
            region_name=os.environ.get("AWS_REGION"),
        )
        bucket_name = os.environ.get("S3_BUCKET_NAME")
        if not bucket_name:
            raise ValueError("S3_BUCKET_NAME environment variable not set.")
        s3_client.head_bucket(Bucket=bucket_name)
        logging.info(f"S3 client initialized and connection to bucket '{bucket_name}' verified successfully.")
        return s3_client
    except Exception as e:
        logging.error(f"Failed to create or verify S3 client: {e}")
        raise

# --- Data Loading and Processing ---
def load_and_parse_annotations(annotations_path):
    """Parses the JSON annotation file and returns structured data."""
    logging.info(f"Loading and parsing annotations from {annotations_path}...")
    with open(annotations_path, 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    data = {"audio_path": [], "transcription": []}
    skipped_count = 0
    for task in annotations:
        s3_uri = task.get("data", {}).get("audio")
        if not s3_uri:
            skipped_count += 1
            continue
        segments = []
        if task.get("annotations"):
            for result in task["annotations"][0].get("result", []):
                if result.get("from_name") == "transcription":
                    start_time = result.get("value", {}).get("start", 0)
                    text = " ".join(result.get("value", {}).get("text", [])).strip()
                    label = "unknown"
                    for label_result in task["annotations"][0].get("result", []):
                        if label_result.get("id") == result.get("id") and label_result.get("from_name") == "label":
                           label = " ".join(label_result.get("value",{}).get("labels",[]))
                           break
                    if "noise" not in label.lower() and text:
                        segments.append((start_time, text))
        if not segments:
            skipped_count += 1
            continue
        segments.sort(key=lambda x: x[0])
        full_transcription = " ".join([text for start_time, text in segments])
        data["audio_path"].append(s3_uri)
        data["transcription"].append(full_transcription)
    if skipped_count > 0:
        logging.warning(f"Skipped {skipped_count} tasks due to missing audio URI or transcriptions.")
    logging.info(f"Successfully parsed {len(data['audio_path'])} valid data entries.")
    return Dataset.from_dict(data)

# --- Data Augmentation ---
augment_pipeline = Compose([
    Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.3),
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),
    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.2),
    PitchShift(min_semitones=-2, max_semitones=2, p=0.2),
])

# --- Progress Logging and Processing Function ---
class ProgressTracker:
    def __init__(self, total, name=""):
        self.count = 0
        self.total = total
        self.name = name
        self.lock = threading.Lock()

    def increment(self):
        with self.lock:
            self.count += 1
        if self.count % 10 == 0 or self.count == self.total:
            logging.info(f"  --> [{self.name}] Processed {self.count}/{self.total} audio files...")

def process_and_tokenize_audio(batch, processor, s3_client, tracker, is_training=False):
    """Downloads, processes, tokenizes audio, and updates progress."""
    s3_uri = batch["audio_path"]
    bucket = s3_uri.split('/')[2]
    key = '/'.join(s3_uri.split('/')[3:])
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key)
        audio_bytes = response['Body'].read()
        audio_array, _ = librosa.load(io.BytesIO(audio_bytes), sr=16000, mono=True)
        if is_training:
            audio_array = augment_pipeline(samples=audio_array, sample_rate=16000)
        
        # Prepare all features needed by the trainer
        batch["input_features"] = processor.feature_extractor(audio_array, sampling_rate=16000).input_features[0]
        batch["labels"] = processor.tokenizer(batch["transcription"]).input_ids
        tracker.increment()
        return batch
    except Exception as e:
        logging.error(f"Failed to process {s3_uri}: {e}")
        return None

def main():
    # Check if data is already processed
    if os.path.exists(PROCESSED_TRAIN_PATH) and os.path.exists(PROCESSED_VAL_PATH):
        logging.info("Pre-processed data already found on disk. Skipping data preparation.")
        return

    s3_client = get_s3_client()
    processor = WhisperProcessor.from_pretrained(BASE_MODEL_PATH, language="vi", task="transcribe")
    
    raw_dataset = load_and_parse_annotations(ANNOTATIONS_FILE_IN_CONTAINER)
    dataset_dict = raw_dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset_raw = dataset_dict["train"]
    val_dataset_raw = dataset_dict["test"]

    # Initialize trackers for detailed logging
    train_tracker = ProgressTracker(total=len(train_dataset_raw), name="Training")
    val_tracker = ProgressTracker(total=len(val_dataset_raw), name="Validation")

    logging.info(f"Processing {len(train_dataset_raw)} training samples with augmentation...")
    train_dataset = train_dataset_raw.map(
        lambda x: process_and_tokenize_audio(x, processor, s3_client, train_tracker, is_training=True),
        num_proc=4, remove_columns=train_dataset_raw.column_names
    )

    logging.info(f"Processing {len(val_dataset_raw)} validation samples...")
    val_dataset = val_dataset_raw.map(
        lambda x: process_and_tokenize_audio(x, processor, s3_client, val_tracker, is_training=False),
        num_proc=4, remove_columns=val_dataset_raw.column_names
    )
    
    logging.info("Saving processed datasets to disk for future runs...")
    train_dataset.save_to_disk(PROCESSED_TRAIN_PATH)
    val_dataset.save_to_disk(PROCESSED_VAL_PATH)
    logging.info("Data preparation complete.")

if __name__ == "__main__":
    main()
Step 3: The train_model.py Script

This script is now much simpler. It doesn't know about S3 or audio files, only about the pre-processed data on disk.

code
Python
download
content_copy
expand_less
# train_model.py

import os
import logging
from dataclasses import dataclass

import torch
import numpy as np
from datasets import load_from_disk
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    utils,
)
from peft import PeftModel, prepare_model_for_kbit_training
import jiwer

# --- Basic Setup ---
utils.logging.set_verbosity_error()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration Paths ---
BASE_MODEL_PATH = "./model"
ADAPTER_TO_CONTINUE_FROM = "./my-whisper-medium-lora"
NEW_ADAPTER_SAVE_PATH = "/app/outputs/vietbud500_adapter_final"
# The input directories for our cached, processed data
PROCESSED_TRAIN_PATH = "/app/processed_data/train"
PROCESSED_VAL_PATH = "/app/processed_data/validation"

# --- Data Collator and Metrics (No changes needed) ---
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: WhisperProcessor
    def __call__(self, features):
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        if (labels[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

def compute_metrics(pred):
    pred_ids = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    wer = jiwer.wer(label_str, pred_str)
    return {"wer": wer}

def main():
    # 1. Load Processor
    processor = WhisperProcessor.from_pretrained(BASE_MODEL_PATH, language="vi", task="transcribe")

    # 2. Load Pre-processed Datasets from Disk (This is super fast!)
    logging.info(f"Loading pre-processed training data from {PROCESSED_TRAIN_PATH}...")
    train_dataset = load_from_disk(PROCESSED_TRAIN_PATH)
    logging.info(f"Loading pre-processed validation data from {PROCESSED_VAL_PATH}...")
    val_dataset = load_from_disk(PROCESSED_VAL_PATH)
    logging.info("Datasets loaded successfully.")

    # 3. Model Loading and Hybrid Fine-Tuning Setup
    logging.info("Loading base model and applying LoRA adapter...")
    model = WhisperForConditionalGeneration.from_pretrained(
        BASE_MODEL_PATH, device_map="auto", use_cache=False, torch_dtype=torch.bfloat16
    )
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True, gradient_checkpointing_kwargs={"use_reentrant": False})
    model = PeftModel.from_pretrained(model, ADAPTER_TO_CONTINUE_FROM, is_trainable=True)

    logging.info("Unfreezing convolutional layers for hybrid fine-tuning...")
    for name, param in model.model.model.encoder.named_parameters():
        if "conv" in name:
            param.requires_grad = True
    model.print_trainable_parameters()

    # 4. Training Arguments
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    has_bf16 = torch.cuda.is_bf16_supported()
    training_args = Seq2SeqTrainingArguments(
        output_dir=NEW_ADAPTER_SAVE_PATH,
        per_device_train_batch_size=16,
        gradient_accumulation_steps=4,
        learning_rate=5e-6,
        warmup_steps=50,
        num_train_epochs=5,
        bf16=has_bf16,
        fp16=not has_bf16,
        optim="adamw_torch",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        logging_steps=10,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        dataloader_num_workers=4,
        generation_max_length=448,
    )

    # 5. Initialize and Run Trainer
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        processor=processor,
    )

    logging.info("Starting model training...")
    trainer.train()

    logging.info("Final evaluation on the best model:")
    final_metrics = trainer.evaluate()
    logging.info(f"Final WER: {final_metrics['eval_wer']:.4f}")

    logging.info(f"Training complete. Best adapter saved to {NEW_ADAPTER_SAVE_PATH}")
    trainer.save_model()

if __name__ == "__main__":
    main()
Step 4: The run_pipeline.sh Orchestrator

This simple script will be the new entrypoint for your container.

code
Bash
download
content_copy
expand_less
#!/bin/bash
# This script runs the two main stages of the pipeline in sequence.
# It uses 'set -e' to ensure that the script will exit immediately if any command fails.
set -e

echo "========================================="
echo "      STARTING FINE-TUNING PIPELINE      "
echo "========================================="

echo ""
echo "--- Step 1: Preparing Data (Downloading, Augmenting, Caching) ---"
python3 prepare_data.py

echo ""
echo "--- Step 2: Training Model (Loading Cached Data & Fine-Tuning) ---"
python3 train_model.py

echo ""
echo "========================================="
echo "        PIPELINE COMPLETED ðŸŽ‰"
echo "========================================="

IMPORTANT: Make this script executable on your host machine:

code
Bash
download
content_copy
expand_less
chmod +x run_pipeline.sh
Step 5: Final Update to docker-compose.yml

We just need to add the new volume mount for processed_data and change the command.

code
Yaml
download
content_copy
expand_less
version: '3.8'

services:
  finetuner:
    image: finetune_whisper_lora:v3
    container_name: lora-finetuning-job-s3
    user: "${HOST_UID}:${HOST_GID}"
    env_file:
      - .env
    environment:
      - HF_HOME=/app/.cache/huggingface
      - TORCH_HOME=/app/.cache/torch
      - AWS_CA_BUNDLE=/app/ca-bundle.crt
      - TRITON_CACHE_DIR=/app/.cache/triton
      - NUMBA_CACHE_DIR=/app/.cache/numba
    working_dir: /app
    volumes:
      - .:/app
      - ./.cache:/app/.cache
      - ${ANNOTATIONS_FILE_PATH}:${ANNOTATIONS_FILE_IN_CONTAINER}:ro
      - ${S3_CA_BUNDLE_PATH}:/app/ca-bundle.crt:ro
      - /etc/passwd:/etc/passwd:ro
      - /etc/group:/etc/group:ro
      - ./outputs:/app/outputs
      ### NEW ###
      # Mount the directory for our cached, pre-processed data
      - ./processed_data:/app/processed_data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['${MIG_DEVICE_UUID}']
    ### MODIFIED ###
    # The command is now our pipeline orchestrator script.
    command: /bin/bash run_pipeline.sh
Your New, Improved Workflow

Run the full pipeline:

code
Bash
download
content_copy
expand_less
# Make sure to stop any old containers first
docker-compose down

# Run the full pipeline
HOST_UID=$(id -u) HOST_GID=$(id -g) docker-compose up

The first time, it will run both prepare_data.py (which will be slow) and then train_model.py. You will see detailed progress logs for the data preparation step.

Re-run only the training:
Let's say the training fails or you want to change a training argument.

Stop the container (docker-compose down or Ctrl+C).

Make your changes to train_model.py.

Run the command again: HOST_UID=$(id -u) HOST_GID=$(id -g) docker-compose up.

This time, prepare_data.py will start, see that the processed_data directory is already full, print a message saying "Skipping data preparation," and finish instantly. The pipeline will then immediately start train_model.py, getting you back to training in seconds.
