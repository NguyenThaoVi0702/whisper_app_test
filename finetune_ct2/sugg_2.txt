
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | ==========
lora-finetuning-job-s3  | == CUDA ==
lora-finetuning-job-s3  | ==========
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | CUDA Version 12.0.1
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | This container image and its contents are governed by the NVIDIA Deep Learning Container License.
lora-finetuning-job-s3  | By pulling and using the container, you accept the terms and conditions of this license:
lora-finetuning-job-s3  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | =========================================
lora-finetuning-job-s3  |       STARTING FINE-TUNING PIPELINE
lora-finetuning-job-s3  | =========================================
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | --- Step 1: Preparing Data (Downloading, Augmenting, Caching) ---
lora-finetuning-job-s3  | 2025-10-31 10:07:06,136 - INFO - Pre-processed data already found on disk. Skipping data preparation.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | --- Step 2: Training Model (Loading Cached Data & Fine-Tuning) ---
lora-finetuning-job-s3  | 2025-10-31 10:07:11,696 - INFO - Loading pre-processed training data from /app/processed_data/train...
lora-finetuning-job-s3  | 2025-10-31 10:07:11,705 - INFO - Loading pre-processed validation data from /app/processed_data/validation...
lora-finetuning-job-s3  | 2025-10-31 10:07:11,707 - INFO - Datasets loaded successfully.
lora-finetuning-job-s3  | 2025-10-31 10:07:11,707 - INFO - Loading base model and applying LoRA adapter...
lora-finetuning-job-s3  | 2025-10-31 10:07:11,979 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
lora-finetuning-job-s3  | 2025-10-31 10:07:13,042 - INFO - Unfreezing convolutional layers for hybrid fine-tuning...
lora-finetuning-job-s3  | Traceback (most recent call last):
lora-finetuning-job-s3  |   File "/app/train_model.py", line 128, in <module>
lora-finetuning-job-s3  | trainable params: 11,962,880 || all params: 815,431,680 || trainable%: 1.4671
lora-finetuning-job-s3  |     main()
lora-finetuning-job-s3  |   File "/app/train_model.py", line 107, in main
lora-finetuning-job-s3  |     trainer = Seq2SeqTrainer(
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
lora-finetuning-job-s3  |     return func(*args, **kwargs)
lora-finetuning-job-s3  | TypeError: Seq2SeqTrainer.__init__() got an unexpected keyword argument 'processor'
lora-finetuning-job-s3 exited with code 1
