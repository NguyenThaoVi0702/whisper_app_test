Of course. A Word Error Rate (WER) of 0.89 is a clear sign that there is a fundamental disconnect between the data being fed to the model and what it expects. This is not a hyperparameter issue; it is a data pipeline issue.

After a thorough review of your prepare_data.py script, I have identified the most likely cause and several other areas for significant improvement.

The core problem is almost certainly a lack of text normalization.

The Core Problem: Text Normalization

The Whisper model was pre-trained on a massive dataset where the transcription text was heavily cleaned and normalized. This typically includes:

Converting all text to lowercase.

Removing all punctuation (commas, periods, question marks, etc.).

Spelling out numbers (e.g., "100" -> "one hundred").

Removing special characters and symbols.

Your script takes the raw transcription text directly from your annotations and tokenizes it:
batch["labels"] = processor.tokenizer(batch["transcription"]).input_ids

If your transcriptions contain any casing, punctuation, or other characters the model didn't frequently see during pre-training, it creates a vocabulary mismatch. The model is being penalized for predicting "hello world" when the label is "Hello, world.", leading to a catastrophic WER.

Action Plan to Drastically Improve Your WER

Follow these steps in order. Step 1 is mandatory and will likely provide the biggest improvement.

Step 1: Implement Aggressive Text Normalization (Highest Priority)

You must clean your transcriptions before tokenizing them. Let's create a normalization function and insert it into your processing pipeline.

1. Add a Normalization Function:
Create a function that converts text to lowercase and removes all characters except for Vietnamese letters and spaces.

code
Python
download
content_copy
expand_less
# Add this function near the top of your prepare_data.py script
import re

# This regex keeps only lowercase Vietnamese letters and spaces.
# It will remove punctuation, numbers, and other symbols.
vietnamese_chars = "a-zàáâãèéêìíòóôõùúýăđĩũơưạảấầẩẫậắằẳẵặẹẻẽếềểễệỉịọỏốồổỗộớờởỡợụủứừửữự"
def normalize_transcription(text):
    text = text.lower()
    text = re.sub(f'[^{vietnamese_chars}\s]', '', text)
    # Collapse multiple spaces into one
    text = re.sub(r'\s+', ' ', text).strip()
    return text

2. Modify Your process_and_tokenize_audio Function:
Apply this normalization function right before you tokenize the labels.

code
Python
download
content_copy
expand_less
def process_and_tokenize_audio(batch, processor, tracker, is_training=False):
    """Downloads, processes, tokenizes audio, and updates progress."""
    global s3_client 
    if s3_client is None:
        s3_client = get_s3_client_for_worker()
        
    s3_uri = batch["audio_path"]
    bucket = s3_uri.split('/')[2]
    key = '/'.join(s3_uri.split('/')[3:])
    try:
        response = s3_client.get_object(Bucket=bucket, Key=key)
        audio_bytes = response['Body'].read()
        audio_array, _ = librosa.load(io.BytesIO(audio_bytes), sr=16000, mono=True)
        
        if is_training:
            audio_array = augment_pipeline(samples=audio_array, sample_rate=16000)
            
        batch["input_features"] = processor.feature_extractor(audio_array, sampling_rate=16000).input_features[0]
        
        ### ================================================================== ###
        ### CRITICAL FIX: Normalize the transcription before tokenizing.       ###
        ### ================================================================== ###
        normalized_text = normalize_transcription(batch["transcription"])
        batch["labels"] = processor.tokenizer(normalized_text).input_ids
        
        tracker.increment()
        return batch
    except Exception as e:
        logging.error(f"Failed to process {s3_uri}: {e}")
        # Return a structure that can be filtered out later if needed
        return None

After making this change, you will need to delete your old processed data cache (/app/processed_data) and re-run prepare_data.py. Then, re-run your training.

Step 2: Manually Verify Data Quality

A WER of 0.89 could also mean some of your audio files are mismatched with their transcriptions. You must perform a manual sanity check.

Create a small script to load a few examples from your processed dataset, listen to the audio, and print the normalized transcription.

code
Python
download
content_copy
expand_less
# check_data.py
from datasets import load_from_disk
import soundfile as sf
import random

PROCESSED_TRAIN_PATH = "./processed_data/train" # Adjust path as needed

# Load the processed dataset
dataset = load_from_disk(PROCESSED_TRAIN_PATH)
processor = WhisperProcessor.from_pretrained("./model", language="vi", task="transcribe")

for _ in range(5): # Check 5 random samples
    random_idx = random.randint(0, len(dataset) - 1)
    sample = dataset[random_idx]
    
    # Decode the labels back to text
    transcription = processor.tokenizer.decode(sample["labels"], skip_special_tokens=True)
    
    # The input_features are just a list, we can't directly play it back.
    # This check is primarily for the text.
    print("-" * 50)
    print(f"Sample Index: {random_idx}")
    print(f"Normalized Transcription: '{transcription}'")
    print("-" * 50)

# This will show you what the model is actually seeing as the ground truth.
# Does it look like clean, correct Vietnamese?

This check will confirm that your normalization is working and that the text itself is reasonable.

Step 3: Tune Hyperparameters for Better Performance

Now that your data is clean, you can focus on training better. A WER of 0.89 means your previous training run was essentially learning garbage. With clean data, these changes will now have a real impact.

Increase the Learning Rate: 5e-6 is quite low for LoRA. A common and effective learning rate for LoRA fine-tuning is 1e-4. This will help the model learn much faster.

Re-enable Gradient Checkpointing & Increase Batch Size: Your model is stable now. You should re-enable gradient checkpointing to save VRAM, which will allow you to use a larger batch size. A larger batch size often leads to more stable training and better results.

Changes in train_model.py:

code
Python
download
content_copy
expand_less
# In main()

# Re-enable gradient checkpointing
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)

# ... inside Seq2SeqTrainingArguments ...
training_args = Seq2SeqTrainingArguments(
    # ...
    per_device_train_batch_size=16, # Increase batch size back up
    gradient_accumulation_steps=4,  # Adjust to keep effective batch size high
    learning_rate=1e-4,             # <-- INCREASED LEARNING RATE
    # ...
)```

#### Step 4: Temporarily Disable Augmentation

Your augmentation pipeline seems reasonable, but as a debugging step, it's wise to eliminate variables. Try one training run with **no data augmentation** to establish a baseline.

**Change in `prepare_data.py`:**
Simply comment out the augmentation line in `process_and_tokenize_audio`:
```python
# ...
# if is_training:
#     audio_array = augment_pipeline(samples=audio_array, sample_rate=16000) # <-- Comment this out
# ...

If your performance is much better without augmentation, it might indicate that the base audio quality is low and the augmentations are making it unintelligible. If the performance is similar, you can re-enable it.

Summary of Actions

Fix Normalization (Most Important): Add the normalize_transcription function to prepare_data.py and use it.

Delete Cache & Re-process Data: Remove the old /app/processed_data directory and run the updated preparation script.

Verify Data: Use the checking script to ensure your processed labels are clean and make sense.

Tune Training: Increase the learning rate to 1e-4 and re-enable gradient checkpointing in train_model.py to use a larger batch size.

Re-train the model.

Following these steps, especially the text normalization, should result in a dramatic and immediate improvement in your Word Error Rate.
