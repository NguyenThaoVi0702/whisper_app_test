ai_dev@ppe-nvidia-k8s-worker01:/u01/user-data/vint1/finetune_whisper$ docker exec -it lora-finetuning-job bash
ai_dev@67fefbbf1024:/app$ python3 finetune_lora.py
trainable params: 6,553,600 || all params: 815,431,680 || trainable%: 0.8037
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 199276.89it/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [00:00<00:00, 23103.66files/s]
Generating train split: 634158 examples [02:57, 3570.83 examples/s]^H^H
Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:00<00:00, 209.63it/s]
Generating train split: 7500 examples [00:02, 3417.57 examples/s]
Original train size: 634158
Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 634158/634158 [03:20<00:00, 3161.97 examples/s]
Filtered train size: 633414
Original val size: 7500
Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7500/7500 [00:02<00:00, 3210.73 examples/s]
Filtered val size: 7494
Map:  10%|██████████████▌                                                                                                                                    | 62978/633414 [10:24<42:07, 225.71 examples/s]Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 633414/633414 [1:47:36<00:00, 98.11 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7494/7494 [01:22<00:00, 90.84 examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 633414/633414 [00:00<00:00, 759706.51 examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 633414/633414 [00:04<00:00, 138049.08 examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7494/7494 [00:00<00:00, 622973.23 examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7494/7494 [00:00<00:00, 155121.50 examples/s]
Traceback (most recent call last):
  File "/app/finetune_lora.py", line 162, in <module>
    training_args = Seq2SeqTrainingArguments(
TypeError: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
ai_dev@67fefbbf1024:/app$ ^C
ai_dev@67fefbbf1024:/app$
