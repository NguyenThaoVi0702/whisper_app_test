
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
lora-finetuning-job-s3  |     return self._call_impl(*args, **kwargs)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
lora-finetuning-job-s3  |     return forward_call(*args, **kwargs)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py", line 1069, in forward
lora-finetuning-job-s3  |     layer_outputs = encoder_layer(
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
lora-finetuning-job-s3  |     return self._call_impl(*args, **kwargs)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
lora-finetuning-job-s3  |     return forward_call(*args, **kwargs)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py", line 621, in forward
lora-finetuning-job-s3  |     hidden_states = self.self_attn_layer_norm(hidden_states)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
lora-finetuning-job-s3  |     return self._call_impl(*args, **kwargs)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
lora-finetuning-job-s3  |     return forward_call(*args, **kwargs)
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py", line 217, in forward
lora-finetuning-job-s3  |     return F.layer_norm(
lora-finetuning-job-s3  |   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 2910, in layer_norm
lora-finetuning-job-s3  |     return torch.layer_norm(
lora-finetuning-job-s3  | RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at "/pytorch/c10/cuda/CUDACachingAllocator.cpp":1016, please report a bug to PyTorch.
lora-finetuning-job-s3  | Exception in thread Thread-3 (_pin_memory_loop):
lora-finetuning-job-s3  | Traceback (most recent call last):
lora-finetuning-job-s3  |   File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
lora-finetuning-job-s3 exited with code 1
