ai_dev@ppe-nvidia-k8s-worker01:/u01/user-data/vint1/finetunning_20251031$ docker compose logs -f
WARN[0000] The "HOST_UID" variable is not set. Defaulting to a blank string.
WARN[0000] The "HOST_GID" variable is not set. Defaulting to a blank string.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | ==========
lora-finetuning-job-s3  | == CUDA ==
lora-finetuning-job-s3  | ==========
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | CUDA Version 12.0.1
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | This container image and its contents are governed by the NVIDIA Deep Learning Container License.
lora-finetuning-job-s3  | By pulling and using the container, you accept the terms and conditions of this license:
lora-finetuning-job-s3  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
lora-finetuning-job-s3  |
lora-finetuning-job-s3  | 2025-10-31 09:06:03,879 - INFO - S3 client initialized and connection to bucket 'lhl-s3-a544-aipage-bbh' verified successfully.
lora-finetuning-job-s3  | 2025-10-31 09:06:04,514 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
lora-finetuning-job-s3  | 2025-10-31 09:06:05,583 - INFO - Unfreezing convolutional layers for hybrid fine-tuning...
lora-finetuning-job-s3  | 2025-10-31 09:06:05,587 - INFO - Loading annotations from /app/annotations.json...
lora-finetuning-job-s3  | 2025-10-31 09:06:05,607 - WARNING - Skipped 120 tasks due to missing audio URI or transcriptions.
lora-finetuning-job-s3  | 2025-10-31 09:06:05,607 - INFO - Successfully parsed 397 valid data entries.
lora-finetuning-job-s3  | 2025-10-31 09:06:05,616 - INFO - Processing training dataset with augmentation...
lora-finetuning-job-s3  | Parameter 'function'=<function <lambda> at 0x7f2813753760> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
lora-finetuning-job-s3  | 2025-10-31 09:06:07,728 - WARNING - Parameter 'function'=<function <lambda> at 0x7f2813753760> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
lora-finetuning-job-s3  | trainable params: 11,962,880 || all params: 815,431,680 || trainable%: 1.4671

