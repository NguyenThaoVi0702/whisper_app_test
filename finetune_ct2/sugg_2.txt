Absolutely. Using Docker Compose is an excellent way to manage this setup. It's a declarative, reproducible, and much cleaner approach than a long docker run command in a shell script.

I will provide a complete solution that:

Uses a docker-compose.yml file to define the service.

Uses two separate environment files: secrets.env for your S3 credentials and config.env for non-sensitive configuration, as you requested.

Correctly handles the User ID (UID) and Group ID (GID) to avoid permission issues with your mounted files.

Step 1: Directory Structure

Organize your project files like this. This structure keeps everything clean and separate.

code
Code
download
content_copy
expand_less
.
├── docker-compose.yml
├── secrets.env         <-- S3 secrets go here
├── config.env          <-- Non-secret paths and settings go here
├── your_annotations.json
├── .gitignore
└── your_ca_bundle.crt  <-- Your CA certificate file
Step 2: Create the Environment Files
A) secrets.env (For S3 Credentials)

Create this file and add your sensitive credentials. This file should be kept private and never committed to Git.

code
Ini
download
content_copy
expand_less
# S3 Secret Credentials
# This file should be listed in .gitignore
AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
B) config.env (For Non-Secret Configuration)

Create this file for all other configuration. This makes your docker-compose.yml file cleaner and more reusable.

code
Ini
download
content_copy
expand_less
# Non-Secret Configuration for Fine-Tuning
S3_ENDPOINT_URL=YOUR_S3_URL_ENDPOINT
AWS_REGION=YOUR_BUCKET_REGION

# --- GPU Configuration ---
# Get this from your 'nvidia-smi' output
MIG_DEVICE_UUID=MIG-GPU-a45fc4b1-c85e-5a3b-8b3d-79191377ec06/4/0

# --- Host File Paths ---
# IMPORTANT: Use absolute paths for Compose to avoid ambiguity
S3_CA_BUNDLE_PATH=/path/on/host/to/your_ca_bundle.crt
ANNOTATIONS_FILE_PATH=/path/on/host/to/your_annotations.json

# --- Container File Paths ---
ANNOTATIONS_FILE_IN_CONTAINER=/app/annotations.json

Important: Replace the /path/on/host/to/... with the full, absolute paths to those files on your machine.

Step 3: Create the .gitignore File

Ensure you have a .gitignore file with the following content to prevent committing secrets and cache files.

code
Code
download
content_copy
expand_less
# Ignore environment files containing secrets
secrets.env

# Ignore python cache
__pycache__/

# Ignore cache directories
.cache/
Step 4: Create the docker-compose.yml File

This is the core of the solution. It declaratively defines everything your run_temp.sh script was doing.

code
Yaml
download
content_copy
expand_less
version: '3.8'

services:
  finetuner:
    # Use the same image as before
    image: finetune_whisper_lora:v3
    container_name: lora-finetuning-job-s3

    # This is how we handle the User ID and Group ID.
    # We will pass these variables from the host machine before running compose.
    user: "${UID}:${GID}"

    # Load environment variables from our two separate files
    env_file:
      - config.env
      - secrets.env

    # Define other environment variables
    environment:
      - HF_HOME=/app/.cache/huggingface
      - TORCH_HOME=/app/.cache/torch
      - AWS_CA_BUNDLE=/app/ca-bundle.crt

    # Set the working directory inside the container
    working_dir: /app

    # Define all the volumes to be mounted
    volumes:
      # Mount the current project directory to /app
      - .:/app
      # Mount the cache directory
      - ./.cache:/app/.cache
      # Mount the annotations file (read-only)
      # The host path is read from the config.env file
      - ${ANNOTATIONS_FILE_PATH}:${ANNOTATIONS_FILE_IN_CONTAINER}:ro
      # Mount the CA certificate (read-only)
      - ${S3_CA_BUNDLE_PATH}:/app/ca-bundle.crt:ro
      # Mount user/group files for correct user mapping
      - /etc/passwd:/etc/passwd:ro
      - /etc/group:/etc/group:ro

    # This section is for GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              # The MIG device ID is read from the config.env file
              device_ids: ['${MIG_DEVICE_UUID}']

    # The command to execute when the container starts
    command: python3 finetune_lora.py
Step 5: How to Use It

Your workflow is now much simpler.

Fill in your configuration:

Open secrets.env and add your S3 keys.

Open config.env and fill in your S3 URL, region, MIG device UUID, and the absolute paths to your CA certificate and annotations JSON file.

Run from your terminal:

First, export your current user and group ID so Docker Compose can use them. This is the crucial step for permissions.

code
Bash
download
content_copy
expand_less
export UID=$(id -u)
export GID=$(id -g)

Then, start the container in detached (background) mode:

code
Bash
download
content_copy
expand_less
docker-compose up -d

Monitor the progress:

To see the logs of your training job, just run:

code
Bash
download
content_copy
expand_less
docker-compose logs -f

Stop and clean up:

When the training is done (or if you want to stop it), you can stop and remove the container with a single command:

code
Bash
download
content_copy
expand_less
docker-compose down
Breakdown of the docker-compose.yml

services:: Defines the different containers in your application. We only have one, which we've named finetuner.

user: "${UID}:${GID}": This is the solution for permissions. It tells the container to run processes as the user who ran the docker-compose command, ensuring that any files created in mounted volumes (like your saved model adapter) will be owned by you, not root.

env_file:: A list of files from which to load environment variables. This keeps your main Compose file clean.

volumes:: This section maps directories and files from your host machine into the container. The :ro flag means "read-only," which is a good security practice for files that the container shouldn't modify (like your annotations and certs).

deploy:: This is the modern way to request resources like GPUs in Docker Compose. It's more structured than the old --gpus flag.

command:: Specifies the default command to run when the container starts, overriding any CMD in the Dockerfile.
